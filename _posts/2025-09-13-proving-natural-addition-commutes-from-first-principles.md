---
title: Proving Natural Addition Commutes from First Principles
date: 2025-09-13 01:00:00 +1000

categories: [Technical]
tags: [lean,maths,computer-science,technical,type-theory,computer-science]
image:
    path: /assets/img/building_blocks.jpg
---

This article is the first installment in a series on type theory. Type theory is a field of math that is not typically covered in an undergrad math or CS degree, but is quite beginner friendly. When talking about it with friends I often draw parallels between it and category theory since they are both closely related to programming/are CS-adjacent. They are also both learnable without a formal background in math. In fact, [mathematics itself can be founded on either of these](https://www.andrew.cmu.edu/user/awodey/preprints/stcsFinal.pdf) without the need to involve sets. Yet, to encounter either field in classical (not CS) mathematical contexts one must traverse a vast array of terrains over years of study peaking in advanced algebraic topology/homotopy theory. And the reason this is is that to truly appreciate the purpose and motivation of either of these fields you must study examples in areas of math that someone without significant academic maturity would not understand. The prototypical motivating example of category theory is the deliberately abstract product construction- which to understand requires that a student has experienced various instances of products from domains such as graph theory, group theory, set theory and topology, after which the student can of their own see the need for a common unifying definition.

With that being said, we won't cover anything that advanced immediately. So buckle up, let's introduce type theory.

## Type Theory

> When I was studying theoretical computer science my Professor, Dr Luke Matthieson, told me that us computer scientists were lucky because unlike other mathematicians we had a fully realised model of the thing we were studying- a Turing machine. I remember at the time having reservations about that since a computer (while computationally equivalent to, modelled from, and functionally able to simulate a Turing machine) does not help prove things like decidability or completeness which is what we were trying to do. TCS is about abstract models and is machine independent. Well with type theory, we actually do have useful realised models. There are many type theories just as there are many set theories, dependening on which axioms one assumes. And for each of the main theories there are what we call *dependently typed programming languages* that for all practical purposes are pure implementations of the theories. And unlike a Turing machine, these languages are very much relied on both to better understand the field of type theory experimentally *and* as a tool for formally verifying, studying and developing, all maths- including, of course, type theory itself.

Type theory is the study of types and terms. It also includes the rules for constructing and reasoning about types and terms. A type is an abstract thing. All terms belong to (or are 'of') a type. This much must be taken for granted, similar to how we just accept sets without thinking too hard about what membership really means ([read this dialogue to learn about the difficulty of using logic and reasoning to defend itself](https://www.physixfan.com/wp-content/files/GEBen.pdf#page=199)). Similar to the variety of set theories such as ZFC, ZF, and SP, we have many type theories. The theory we are going to be studying today is the Calculus of Inductive Constructions (CIC), a variant of the Calculus of Constructions (CoC) with the added benefit of inductive types.

### A Note on Terms and Programs

Type theory often substitutes the word "terms" with "programs". Loosely speaking, a program is a set of instructions. If you have written code before, computer programs in functional languages (e.g. Haskell functions, Nix flakes) are examples of programs because they always return something. Consider the minimal program `2 + 3` in a language such as Haskell. The type of this program is `Int` because it evaluates to the term `5`, which is of type `Int`. That the program evaluates to a fixed value is the same as it "returning" or "outputting" that value. Similarly to terms, programs are categorised by their types. The program `7` is different to the program `2 + 3` but they have the same type. The question of whether `2 + 3` and `5` are the same program is a little bit harder to answer but the academic consensus is yes, as [they reduce to the same normal form](https://lean-lang.org/doc/reference/latest/The-Type-System/#:~:text=In%20addition%20to%20having%20types%2C%20terms%20are%20also%20related%20by%20definitional%20equality.%20This%20is%20the%20mechanically%2Dcheckable%20relation%20that%20syntactically%20equates%20terms%20modulo%20their%20computational%20behavior.%20Definitional%20equality%20includes%20the%20following%20forms%20of%20reduction%3A). 

## The Curry-Howard Correspondence

The Curry-Howard correspondence (CH) is a powerful association that asserts that types and programs of a type in type theory are isomorphic to propositions and proofs of a proposition in formal logic. Take the proposition `P` that double an odd number is even. We can prove this algebraically by constructing an odd number, algorithmically doubling it and matching the result against the form of an even number. According to CH, this process is semantically identical to declaring a type `P` and constructing a term of the type `P`. For example, the `Int` type could correspond to this proposition. Therefore, by declaring a function that returns integers and implementing it, to return an integer, we are 'proving' `P`. It sounds a little hand wavey- can't I declare that `Int` corresponds to the proposition that `2 + 2 = 5`, or something else wrong? Let's define some rules to explain what's required from you and the language you are working on to show why our hypothetical `Int` function is (relatively) meaningless and how to construct meaningful types such as `2 + 2 = 4` as a type.

> For this next section I use `P` to mean proposition and type interchangeably

#### The Language Must Be Statically Typed

The compiler has to verify that the function really does produce an element of type `P`. This is already an industry standard in robust enterprise languages like C#. At compile time the code is parsed into an AST and the symbols are resolved (mapped to types) and type checked. Now getting our program to compile is proof that we have produced a term of this type, and proven `P`, even if we don't use the result of the function. The technical way to describe this is that the type `P` is *inhabited*. But big deal, we know that `Int`s exist! Maybe if the type were more exotic its existence would be noteworthy...

#### Our Types Need to Be Expressive

`Int` is about as simple a type as you can get. The same goes for `Bool` and `String` and `Console` and `Animal` and really any type that would be declared `public class MyClass {}` in C#. Proving that these are inhabited is neither difficult nor useful. The way to build complex types is to create types that have information built into them:

1) Parametric Polymorphism (C# Generics) allow you to index a type using another type. For example, the `List` type in C#. `List` is a type constructor, and has *kind* (a kind is like the type of a signature) `Type -> Type` which means it takes a type `T` and produces a new type `List<T>` (it constructs types). `Type` here is the type of types, it is inhabited by terms such as `Int` and `Bool`. These have terms themselves- we are beginning to build hierarchies similar to sets of sets, but with more information encoded in the hierarchy.

2) Dependent types (no C# equivalnet) allow you to index a type with a value, to create a type constructor that uses a value to create another type. Imagine if the `List` above accepted 2 parameters, a type `T` and a concrete `int` representing how long it would be. This would be compile time information stored in the object itself. In dependently typed languages this is a standard type and is usually given the name `Vec`. The kind of `Vec` is `Type -> Nat -> Type`. Now when we want to return a `Vec` we must specify not just the type of elements in it as we would in C# but the length of it too- this is critical type information and must be respected to preserve type safety. This is more powerful than C++ template params, because the paramtere does not need to be a compile time constant. All of a sudden our types are more powerful!

> Side note
{: .prompt-info }

If you are not familiar with the arrow syntax, it is common in functional languages to denote function signatures/type kinds. If `A` is a type and `B` is a type, then `A -> B` is a type representing a function that accepts `A` and returns `B`. The chaining of arrows appears when we want to denote multiple input parameters. The `->` operator is right associative so `A -> B -> C` is interpreted as `A -> (B -> C)`. This makes the expression a function that accepts `A` and returns (a function accepting `B` and returning `C`). You can provide both parameters by invoking it twice first with `A` then with `B`. Functions can't return more than one thing- if you desperately need to return many things create a record/tuple/product type and return a single instance of that. From this perspective functions take and return one thing, which is conceptually fundamental. The tradeoff is that we ended up passing functions into and getting them from other functions (e.g. our return type above was a function of type `B -> C`). We have naturally arrived at 2 core tenets of functional programming (FP). Firstly functions that manipulate functions exist and they are called higher order functions. Secondly the process of restructuring a multi-param function to take one parameter at a time and pass multiple parameters via successive invocations is called currying. 

3) $$\Sigma$$ types and $$\Pi$$ types, also known as the dependent pair and dependent function describe the type of a tuple/function where the type of the second elemenet in the pair/output changes on the value of the first element of the pair/value of the input. For example, when `printf` is provided the string literal `My age is %d My favourite letter is %c` as an argument its signature changes to require an integer and a char. However, if it were called with argument `Hello, World!` it would require no additional arguments. `printf` is an example of dependently typed functions in the C language. But while `printf` is hardcoded into the language, languages with actual support for dependent types let us build these structures from scratch. Phrased more technically, the dependent pair and dependent function can be described as $$(X, P(X))$$ and $$X \to P(X)$$ respectively, where $$P$$ is a function that accepts a value and returns a type. It is a function whose domain is the set of all elements from the set of values of all types and whose codomain is the set of types. If you happened to notice that the previous definition assumed the existence of sets, please don't think too hard about it.

4) Inductive Data Types (ADTs/GADTs) are an entire topic on their own, but essentially allow you to create data types that are constructed from instances of the same data type. For example in C#, having a constructor that requires an instance of the class as a parameter. Unlike in C#, the compiler is aware of all of the finitely many ways to construct an object and lets you pattern match on the constructor to choose how to treat an instance in a function depending on how it was created. Inductive types can return different types depending on which data constructor (FP constructors are called data constructors) was called to create the instance. So... give example of this

We can now generate very complex types, such as type safe expressions `Expr` built recursively from subexpressions, type safe `printf` that knows how to change its signature based on the provided argument, `Tree`s and `List`s become trivial and we can model the simply typed lambda calculus (STLC) in a type safe way. This is incredible and already allows us much more flexibility in writing code contracts and describing type data for day-to-day purposes. But we started this tangent with the purpose of showing how terms of a type constitute proofs of a proposition. And so far all we can do is show that any of our exotic types are inhabited. How does this prove anything? Well, we need to find a way to encode logical meaning into our types.

#### Our Types Don't Mean Anything (Yet)

logical connectives are codifications of patterns of thought
= connector